{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bog-9ygSgek6"
      },
      "source": [
        "# Optimizing the model with series of experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9wLOENogek9"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEeiI5Ekgek-"
      },
      "source": [
        "### LetÂ´s first reload the data from the previous notebook. Then, perform train_test_split once again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHwaGfg_gek_"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = pathlib.Path.cwd().parent / 'data'\n",
        "print(DATA_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGH-oL7rgek_"
      },
      "outputs": [],
      "source": [
        "model_data_scaled_path = DATA_DIR / 'processed' / 'ames_model_data_scaled.pkl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7Y5Iacjgek_"
      },
      "outputs": [],
      "source": [
        "data = pd.read_pickle(model_data_scaled_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jj5g9uDugek_"
      },
      "outputs": [],
      "source": [
        "X = data.drop(columns=['SalePrice']).copy().values\n",
        "y = data['SalePrice'].copy().values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siGQ3G7ggelA"
      },
      "outputs": [],
      "source": [
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nm7_WOdgelA"
      },
      "outputs": [],
      "source": [
        "RANDOM_SEED = 42  # Any number here, really.\n",
        "\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.25,\n",
        "    random_state=RANDOM_SEED,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9fYbEhHgelA"
      },
      "source": [
        "### Experiment 1: LinearRegression with transformed values\n",
        "\n",
        "Our first experiment is to rerun the linear regression model with the same features as before, but this time on the new data with the transformations applied on the previous notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5msGTsiwgelB"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wesTDvkUgelB"
      },
      "outputs": [],
      "source": [
        "linear_scaled_model = LinearRegression()\n",
        "\n",
        "linear_scaled_model.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this and the following experiments, we will also use cross_val_score instead of simply predicting on the test set. This will give us a better idea of how the model will perform on unseen data, given that we are making more realistic performance estimates, and the variance of the model evaluation will be lower."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAh5ZQ6CgelB"
      },
      "outputs": [],
      "source": [
        "scores_linear_scaled = cross_val_score(linear_scaled_model,\n",
        "                                       Xtrain,\n",
        "                                       ytrain,\n",
        "                                       cv=10,\n",
        "                                       scoring='neg_mean_squared_error',\n",
        "                                       n_jobs=-1)\n",
        "\n",
        "scores_linear_scaled = np.sqrt(-scores_linear_scaled)\n",
        "error_percent_linear = 100 * (10**scores_linear_scaled.mean() - 1)\n",
        "std_percent_linear = 100 * (10**scores_linear_scaled.std() - 1)\n",
        "print(f'Average error is {error_percent_linear:.2f}%')\n",
        "print(f'Standard deviation of error is {std_percent_linear:.2f}%')\n",
        "\n",
        "path = DATA_DIR / 'processed' / 'linear_scaled_model.csv'\n",
        "\n",
        "# write the array scores_linear_scaled to a csv file\n",
        "\n",
        "np.savetxt(path, scores_linear_scaled, delimiter=',')\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWJxJtLFgelB"
      },
      "source": [
        "The experiment showed no large difference in the model performance, considering the standard deviation. This means that the LinearRegression model is not sensitive to the new transformations we made on the data. We will, however, still keep the new data for the next experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO88AXX_gelB"
      },
      "source": [
        "### Experiment 2: Lasso Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Lasso Regression is similar to the Linear Regression, but it adds a regularization term to the cost function, which penalizes the model for having too many features (in this case, the L1 norm of the weights). This is useful to avoid overfitting, and also to perform feature selection, since the regularization term will make the weights of the less important features go to zero.\n",
        "\n",
        "We can define this regularization term as:\n",
        "\n",
        "$$\n",
        "\\lambda \\sum_{i=1}^{n} |w_i|\n",
        "$$\n",
        "\n",
        "Where $\\lambda$ is the regularization parameter, and $w_i$ is the weight of the $i$-th feature.\n",
        "\n",
        "As $w_i$ gets closer to zero, the regularization term will also get closer to zero, and the model will be penalized less. This means that the model will try to minimize the cost function by making the weights of the less important features go to zero, and the weights of the most important features will be kept as they are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxfujoYXgelC"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this and the following experiments, we will use GridSearchCV to find better values for model hyperparameters. This will allow us to perform a more thorough search in the hyperparameter space, and find the best model for our data without having to manually test different values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-FnepU-gelC"
      },
      "outputs": [],
      "source": [
        "lasso = Lasso()\n",
        "\n",
        "params = {\n",
        "    'alpha': np.logspace(-4, 0, 100),\n",
        "    'max_iter': [15000, 20000, 30000],\n",
        "    'tol': [0.0001, 0.001],\n",
        "    'selection': ['cyclic', 'random'],\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(lasso, params, cv=5, verbose=1, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5OrzY5PgelC"
      },
      "outputs": [],
      "source": [
        "grid.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get the best parameters for Lasso Regression, from the grid search, and use them to train a new model. Then, evaluate the model using cross_val_score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJIt31ZrgelC"
      },
      "outputs": [],
      "source": [
        "best_params = grid.best_params_\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYjhHl-GgelC"
      },
      "outputs": [],
      "source": [
        "lasso_best = Lasso(**best_params)\n",
        "lasso_best.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8wba6J7gelC"
      },
      "outputs": [],
      "source": [
        "scores_lasso = cross_val_score(lasso_best, Xtrain, ytrain, cv=10, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "scores_lasso = np.sqrt(-scores_lasso)\n",
        "error_percent_lasso = 100 * (10**scores_lasso.mean() - 1)\n",
        "std_percent_lasso = 100 * (10**scores_lasso.std() - 1)\n",
        "print(f'Average error is {error_percent_lasso:.2f}%')\n",
        "print(f'Standard deviation of error is {std_percent_lasso:.2f}%')\n",
        "\n",
        "path = DATA_DIR / 'processed' / 'lasso_score.csv'\n",
        "\n",
        "np.savetxt(path, scores_lasso, delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment 3: Ridge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Ridge Regression is similar to the Linear Regression, but it adds a regularization term to the cost function, which penalizes the model for having too many features (in this case, the L2 norm of the weights). This is useful to avoid overfitting, and also to perform feature selection, since the regularization term will make the weights of the less important features go to zero.\n",
        "\n",
        "We can define this regularization term as:\n",
        "\n",
        "$$\n",
        "\\lambda \\sum_{i=1}^{n} w_i^2\n",
        "$$\n",
        "\n",
        "Where $\\lambda$ is the regularization parameter, and $w_i$ is the weight of the $i$-th feature.\n",
        "\n",
        "Differently from the Lasso Regression, the Ridge Regression will not make the weights of the less important features go to zero, but it will make them very small (since $w_i$ is squared, it will be even smaller than in the Lasso Regression)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ridge = Ridge()\n",
        "\n",
        "params = {\n",
        "    'alpha': np.logspace(-4, 0, 100),\n",
        "    'fit_intercept': [True, False],\n",
        "    'copy_X': [True, False],\n",
        "    'max_iter': [15000, 20000, 30000],\n",
        "    'tol': [0.0001, 0.001],\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(ridge, params, cv=5, verbose=1, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_params = grid.best_params_\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ridge_best = Ridge(**best_params)\n",
        "ridge_best.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scores_ridge = cross_val_score(ridge_best, Xtrain, ytrain, cv=10, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "scores_ridge = np.sqrt(-scores_ridge)\n",
        "error_percent_ridge = 100 * (10**scores_ridge.mean() - 1)\n",
        "std_percent_ridge = 100 * (10**scores_ridge.std() - 1)\n",
        "print(f'Average error is {error_percent_ridge:.2f}%')\n",
        "print(f'Standard deviation of error is {std_percent_ridge:.2f}%')\n",
        "\n",
        "path = DATA_DIR / 'processed' / 'ridge.csv'\n",
        "\n",
        "np.savetxt(path, scores_ridge, delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment 4:  Decision Tree Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Decision trees are a type of algorithm used mostly for classification, since they are easy to interpret and visualize. However, they can also be used for regression. \n",
        "\n",
        "From our exploratory analysis, we know that most variables form a linear relationship, however it is still a good itea to test this model because it is a non-parametric model, which means that it does not make any assumptions about the data distribution. This means that it can capture relationships that linear models cannot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A decision tree is a tree where each node represents a feature (or a group of features), each link represents a decision (resulting from a feature split), and each leaf represents an output (a prediction). The tree is built by splitting the data into subsets, and then splitting it again on each of the subsets, and so on, until the subsets are small enough to be represented by a leaf.\n",
        "\n",
        "As for hyperparameters, we can specify:\n",
        "- min_samples_split: the minimum number of samples required to split an internal node\n",
        "- min_samples_leaf: the minimum number of samples required to be at a leaf node\n",
        "- min_weight_fraction_leaf: the minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node\n",
        "- max_features: the number of features to consider when looking for the best split\n",
        "- min_impurity_decrease: a node will be split if this split induces a decrease of the impurity greater than or equal to this value\n",
        "- ccp_alpha: complexity parameter used for Minimal Cost-Complexity Pruning\n",
        "\n",
        "We will leave max_depth as None, so the nodes will be expanded until all leaves are pure or until all leaves contain less than min_samples_split samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tree = DecisionTreeRegressor()\n",
        "\n",
        "params = {\n",
        "    'min_samples_split': [2, 3, 4],\n",
        "    'min_samples_leaf': [1, 2, 3],\n",
        "    'min_weight_fraction_leaf': [0, 0.01, 0.1],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'min_impurity_decrease': [0, 0.01, 0.1],\n",
        "    'ccp_alpha': [0, 0.01, 0.1]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(tree, params, cv=5, verbose=1, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_params = grid.best_params_\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tree_best = DecisionTreeRegressor(**best_params)\n",
        "tree_best.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scores_tree = cross_val_score(tree_best, Xtrain, ytrain, cv=10, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "scores_tree = np.sqrt(-scores_tree)\n",
        "error_percent_tree = 100 * (10**scores_tree.mean() - 1)\n",
        "std_percent_tree = 100 * (10**scores_tree.std() - 1)\n",
        "print(f'Average error is {error_percent_tree:.2f}%')\n",
        "print(f'Standard deviation of error is {std_percent_tree:.2f}%')\n",
        "\n",
        "path = 'tree_scores.csv'\n",
        "\n",
        "np.savetxt(path, scores_tree, delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLGdIqIEgelD"
      },
      "source": [
        "### Experiment 5:  Random Forest Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The random forest is very similar to the decision tree, but it uses a technique called bagging to reduce the variance of the model, by training many decision trees on different subsets of the data, and then averaging the predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqhuSZ7JgelD"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBn1PDHPgelD"
      },
      "outputs": [],
      "source": [
        "rf = RandomForestRegressor()\n",
        "\n",
        "params = {\n",
        "    'n_estimators': [100, 600, 1200, 1800],\n",
        "    'min_samples_split': [3],\n",
        "    'max_features': ['auto', 'sqrt'],\n",
        "    'verbose': [0, 1, 2, 3],\n",
        "    'max_depth': [None],\n",
        "    'bootstrap': [False],\n",
        "    'min_weight_fraction_leaf': [0.0],\n",
        "    'min_samples_leaf': [1],\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(rf, params, cv=5, verbose=1, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncBy3F-igelD"
      },
      "outputs": [],
      "source": [
        "grid.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqEhl9WZgelD",
        "outputId": "625ba6f2-518f-4c9c-e6c1-45e8f75a6d78"
      },
      "outputs": [],
      "source": [
        "best_params = grid.best_params_\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s88tM9SRgelD"
      },
      "outputs": [],
      "source": [
        "rf_best = RandomForestRegressor(**best_params)\n",
        "rf_best.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwrovDvzgelE"
      },
      "outputs": [],
      "source": [
        "scores_rf = cross_val_score(rf_best, Xtrain, ytrain, cv=10, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "scores_rf = np.sqrt(-scores_rf)\n",
        "error_percent_rf = 100 * (10**scores_rf.mean() - 1)\n",
        "std_percent_rf = 100 * (10**scores_rf.std() - 1)\n",
        "print(f'Average error is {error_percent_rf:.2f}%')\n",
        "print(f'Standard deviation of error is {std_percent_rf:.2f}%')\n",
        "\n",
        "path = DATA_DIR / 'processed' / 'rf_scores.csv'\n",
        "\n",
        "np.savetxt(path, scores_rf, delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHU1A47NgelE"
      },
      "source": [
        "### Experiment 6:  Gradient Boosting Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gradient boosting is a technique that combines weak learners (in this case, decision trees) to create a strong learner. It is similar to the random forest, but instead of training each tree independently, it trains each tree on the residual of the previous tree.\n",
        "\n",
        "We can define the gradient boosting algorithm as:\n",
        "\n",
        "$$\n",
        "F_0(x) = \\underset{\\gamma}{\\arg\\min} \\sum_{i=1}^{n} L(y_i, \\gamma)\n",
        "$$\n",
        "\n",
        "$$\n",
        "F_m(x) = F_{m-1}(x) + \\underset{\\gamma}{\\arg\\min} \\sum_{i=1}^{n} L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))\n",
        "$$\n",
        "\n",
        "Where $F_0(x)$ is the first tree (obtained by minimizing $L(y_i, \\gamma)$ over the training data), $F_m(x)$ is the $m$-th tree, $L$ is the loss function, $y_i$ is the target value, $x_i$ is the $i$-th feature vector, and $h_m(x_i)$ is the prediction of the $m$-th tree.\n",
        "\n",
        "The technique uses the gradient descent algorithm to minimize the loss function, and the trees are added sequentially, so the model is built in a stage-wise fashion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGfMjeHbgelE"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7T6lq5tBgelE"
      },
      "outputs": [],
      "source": [
        "gbr = GradientBoostingRegressor()\n",
        "\n",
        "params = {\n",
        " 'alpha': [0.9], \n",
        " 'criterion': ['friedman_mse'],\n",
        " 'learning_rate': [0.1, 0.5, 0.01], \n",
        " 'max_depth': [3, 4, 5], \n",
        " 'max_features': ['sqrt'], \n",
        " 'min_samples_leaf': [2,3], \n",
        " 'n_estimators': [1600,200], \n",
        " 'subsample': [0.9, 0.8, 0.7, 0.6], \n",
        " 'verbose': [0,1,2], \n",
        " 'warm_start': [True, False]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(gbr, params, cv=5, verbose=1, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQI9A7WXgelE",
        "outputId": "20c2ffb3-69d7-44a2-8145-5c0ca0415bed"
      },
      "outputs": [],
      "source": [
        "grid.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTXDdTvPgelE",
        "outputId": "a1d2b7ba-e649-4f85-ac58-96fda1456de5"
      },
      "outputs": [],
      "source": [
        "best_params = grid.best_params_\n",
        "with open('best_params_gradient.txt', 'w') as f:\n",
        "    f.write(str(best_params))\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMFs95wQgelE",
        "outputId": "5bf2a913-83c3-4c18-f6f6-c5280c713979"
      },
      "outputs": [],
      "source": [
        "gbr_best = GradientBoostingRegressor(**best_params)\n",
        "gbr_best.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JH_u90FUgelE",
        "outputId": "f50ed70e-93f3-450e-c221-b848fbaf92a9"
      },
      "outputs": [],
      "source": [
        "scores_gbr = cross_val_score(gbr_best, Xtrain, ytrain, cv=10, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "scores_gbr = np.sqrt(-scores_gbr)\n",
        "error_percent_gbr = 100 * (10**scores_gbr.mean() - 1)\n",
        "std_percent_gbr = 100 * (10**scores_gbr.std() - 1)\n",
        "print(f'Average error is {error_percent_gbr:.2f}%')\n",
        "print(f'Standard deviation of error is {std_percent_gbr:.2f}%')\n",
        "\n",
        "path = DATA_DIR / 'processed' / 'gradient_scores.csv'\n",
        "\n",
        "np.savetxt(path, scores_gbr, delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_T2JBDzgelF"
      },
      "source": [
        "### Experiment 7:  KNN Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "KNN is a different non-parametric approach to regression. It is a lazy learning algorithm, which means that it does not learn a function from the training data, but instead memorizes the training data (that is, stores everything on memory) and waits until it is given a new data point to make a prediction.\n",
        "\n",
        "During training, the algorithm simply stores the training data. During testing, the algorithm finds the $k$ nearest neighbors of the new data point, and then averages the target values of the neighbors to make a regression prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivWgnFfggelF"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wmDaSRYgelF"
      },
      "outputs": [],
      "source": [
        "knn = KNeighborsRegressor()\n",
        "\n",
        "params = {\n",
        "    'n_neighbors': [5, 6, 7, 8],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "    'p': [1, 2, 3],\n",
        "    'leaf_size': [30, 40, 50]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(knn, params, cv=5, verbose=1, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPB4rVMagelF"
      },
      "outputs": [],
      "source": [
        "best_params = grid.best_params_\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDd4pnOegelJ"
      },
      "outputs": [],
      "source": [
        "knn_best = KNeighborsRegressor(**best_params)\n",
        "knn_best.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scores_knn = cross_val_score(knn_best, Xtrain, ytrain, cv=10, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "scores_knn = np.sqrt(-scores_knn)\n",
        "error_percent_knn = 100 * (10**scores_knn.mean() - 1)\n",
        "std_percent_knn = 100 * (10**scores_knn.std() - 1)\n",
        "print(f'Average error is {error_percent_knn:.2f}%')\n",
        "print(f'Standard deviation of error is {std_percent_knn:.2f}%')\n",
        "\n",
        "path = DATA_DIR / 'processed' / 'knn_scores.csv'\n",
        "\n",
        "np.savetxt(path, scores_knn, delimiter=',')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment 8: ElasticNetCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ElasticNet is a combination of Lasso and Ridge regressions, which adds both regularization terms (L1 and L2) to the cost function. This allows the model to learn a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge.\n",
        "\n",
        "We can define this new cost function as:\n",
        "\n",
        "$$\n",
        "\n",
        "\\lambda_1 \\sum_{i=1}^{n} |w_i| + \\lambda_2 \\sum_{i=1}^{n} w_i^2\n",
        "\n",
        "$$\n",
        "\n",
        "Where $\\lambda_1$ and $\\lambda_2$ are the regularization parameters.\n",
        "\n",
        "It is important to note that ElasticNetCV is not a model, but a method that can be used to find the best values for the regularization parameters $\\lambda_1$ and $\\lambda_2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import ElasticNetCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "elnet = ElasticNetCV()\n",
        "\n",
        "params = {\n",
        "    'l1_ratio': [0.6, 0.3, 0.2, 0.1],\n",
        "    'eps': [1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8],\n",
        "    'n_alphas': [100, 150, 200],\n",
        "    'copy_X': [True, False],\n",
        "    'verbose': [0,1,2]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(elnet, params, cv=5, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_params = grid.best_params_\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "elnet_best = ElasticNetCV(**best_params)\n",
        "elnet_best.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scores_elnet = cross_val_score(elnet_best, Xtrain, ytrain, cv=10, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "scores_elnet = np.sqrt(-scores_elnet)\n",
        "error_percent_elnet = 100 * (10**scores_elnet.mean() - 1)\n",
        "std_percent_elnet = 100 * (10**scores_elnet.std() - 1)\n",
        "print(f'Average error is {error_percent_elnet:.2f}%')\n",
        "print(f'Standard deviation of error is {std_percent_elnet:.2f}%')\n",
        "\n",
        "path = DATA_DIR / 'processed' / 'elnet_scores.csv'\n",
        "\n",
        "np.savetxt(path, scores_elnet, delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwv4f0XOgelK"
      },
      "source": [
        "### Retraining of best model on the whole dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkNrXjEkgelK"
      },
      "outputs": [],
      "source": [
        "best_model = linear_scaled_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-GozjutgelK"
      },
      "outputs": [],
      "source": [
        "best_model.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chossing the best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import ttest_ind\n",
        "\n",
        "def compare_scores(score1, score2):\n",
        "    t_stat, p_value = ttest_ind(score1, score2, equal_var=False)\n",
        "    mean, std = score1.mean(), score1.std()\n",
        "    print(f'T-statistic: {t_stat:.2f}')\n",
        "    print(f'p-value: {p_value:.2f}')\n",
        "    if p_value < 0.5:\n",
        "        print('Model is better than baseline')\n",
        "    else:\n",
        "        print('Results are not significant')\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Linear Scaled Model</th>\n",
              "      <th>Lasso</th>\n",
              "      <th>Ridge</th>\n",
              "      <th>Decision Tree</th>\n",
              "      <th>Random Forest</th>\n",
              "      <th>Gradient Boosting</th>\n",
              "      <th>KNN</th>\n",
              "      <th>Elastic Net</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.072027</td>\n",
              "      <td>0.074151</td>\n",
              "      <td>0.072099</td>\n",
              "      <td>0.098224</td>\n",
              "      <td>0.052702</td>\n",
              "      <td>0.047553</td>\n",
              "      <td>0.074376</td>\n",
              "      <td>0.073719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.044358</td>\n",
              "      <td>0.043487</td>\n",
              "      <td>0.043913</td>\n",
              "      <td>0.082415</td>\n",
              "      <td>0.048090</td>\n",
              "      <td>0.042026</td>\n",
              "      <td>0.073129</td>\n",
              "      <td>0.043525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.045163</td>\n",
              "      <td>0.043955</td>\n",
              "      <td>0.044698</td>\n",
              "      <td>0.079191</td>\n",
              "      <td>0.045496</td>\n",
              "      <td>0.038408</td>\n",
              "      <td>0.074954</td>\n",
              "      <td>0.043873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.073619</td>\n",
              "      <td>0.072708</td>\n",
              "      <td>0.073152</td>\n",
              "      <td>0.097717</td>\n",
              "      <td>0.069103</td>\n",
              "      <td>0.060742</td>\n",
              "      <td>0.087720</td>\n",
              "      <td>0.072671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.049673</td>\n",
              "      <td>0.046664</td>\n",
              "      <td>0.048198</td>\n",
              "      <td>0.084619</td>\n",
              "      <td>0.054938</td>\n",
              "      <td>0.045238</td>\n",
              "      <td>0.070134</td>\n",
              "      <td>0.046859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.040611</td>\n",
              "      <td>0.040680</td>\n",
              "      <td>0.040540</td>\n",
              "      <td>0.082677</td>\n",
              "      <td>0.052312</td>\n",
              "      <td>0.042497</td>\n",
              "      <td>0.074704</td>\n",
              "      <td>0.040572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.051588</td>\n",
              "      <td>0.050013</td>\n",
              "      <td>0.051174</td>\n",
              "      <td>0.092199</td>\n",
              "      <td>0.053761</td>\n",
              "      <td>0.046714</td>\n",
              "      <td>0.077820</td>\n",
              "      <td>0.050328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.042617</td>\n",
              "      <td>0.041405</td>\n",
              "      <td>0.042040</td>\n",
              "      <td>0.068285</td>\n",
              "      <td>0.048096</td>\n",
              "      <td>0.041540</td>\n",
              "      <td>0.072266</td>\n",
              "      <td>0.041271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.046031</td>\n",
              "      <td>0.046415</td>\n",
              "      <td>0.046035</td>\n",
              "      <td>0.089832</td>\n",
              "      <td>0.057975</td>\n",
              "      <td>0.048345</td>\n",
              "      <td>0.079017</td>\n",
              "      <td>0.046150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.056263</td>\n",
              "      <td>0.056120</td>\n",
              "      <td>0.056085</td>\n",
              "      <td>0.082703</td>\n",
              "      <td>0.059759</td>\n",
              "      <td>0.055170</td>\n",
              "      <td>0.079386</td>\n",
              "      <td>0.056046</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Linear Scaled Model     Lasso     Ridge  Decision Tree  Random Forest  \\\n",
              "0             0.072027  0.074151  0.072099       0.098224       0.052702   \n",
              "1             0.044358  0.043487  0.043913       0.082415       0.048090   \n",
              "2             0.045163  0.043955  0.044698       0.079191       0.045496   \n",
              "3             0.073619  0.072708  0.073152       0.097717       0.069103   \n",
              "4             0.049673  0.046664  0.048198       0.084619       0.054938   \n",
              "5             0.040611  0.040680  0.040540       0.082677       0.052312   \n",
              "6             0.051588  0.050013  0.051174       0.092199       0.053761   \n",
              "7             0.042617  0.041405  0.042040       0.068285       0.048096   \n",
              "8             0.046031  0.046415  0.046035       0.089832       0.057975   \n",
              "9             0.056263  0.056120  0.056085       0.082703       0.059759   \n",
              "\n",
              "   Gradient Boosting       KNN  Elastic Net  \n",
              "0           0.047553  0.074376     0.073719  \n",
              "1           0.042026  0.073129     0.043525  \n",
              "2           0.038408  0.074954     0.043873  \n",
              "3           0.060742  0.087720     0.072671  \n",
              "4           0.045238  0.070134     0.046859  \n",
              "5           0.042497  0.074704     0.040572  \n",
              "6           0.046714  0.077820     0.050328  \n",
              "7           0.041540  0.072266     0.041271  \n",
              "8           0.048345  0.079017     0.046150  \n",
              "9           0.055170  0.079386     0.056046  "
            ]
          },
          "execution_count": 130,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# reading the scores from the csv files\n",
        "\n",
        "linear_scaled_model_scores = pd.read_csv(DATA_DIR / 'processed' / 'linear_scaled_model.csv', header=None)\n",
        "lasso_scores = pd.read_csv(DATA_DIR / 'processed' / 'lasso_score.csv', header=None)\n",
        "ridge_scores = pd.read_csv(DATA_DIR / 'processed' / 'ridge.csv', header=None)\n",
        "tree_scores = pd.read_csv('tree_scores.csv', header=None)\n",
        "rf_scores = pd.read_csv(DATA_DIR / 'processed' / 'rf_scores.csv', header=None)\n",
        "gradient_scores = pd.read_csv(DATA_DIR / 'processed' / 'gradient_scores.csv', header=None)\n",
        "knn_scores = pd.read_csv(DATA_DIR / 'processed' / 'knn_scores.csv', header=None)\n",
        "elnet_scores = pd.read_csv(DATA_DIR / 'processed' / 'elnet_scores.csv', header=None)\n",
        "\n",
        "# creating dataframes for the scores\n",
        "\n",
        "scores = pd.DataFrame({'Linear Scaled Model': linear_scaled_model_scores[0],\n",
        "                          'Lasso': lasso_scores[0],\n",
        "                          'Ridge': ridge_scores[0],\n",
        "                          'Decision Tree': tree_scores[0],\n",
        "                          'Random Forest': rf_scores[0],\n",
        "                          'Gradient Boosting': gradient_scores[0],\n",
        "                          'KNN': knn_scores[0],\n",
        "                          'Elastic Net': elnet_scores[0]})\n",
        "\n",
        "scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Linear Scaled Model</th>\n",
              "      <th>Lasso</th>\n",
              "      <th>Ridge</th>\n",
              "      <th>Decision Tree</th>\n",
              "      <th>Random Forest</th>\n",
              "      <th>Gradient Boosting</th>\n",
              "      <th>KNN</th>\n",
              "      <th>Elastic Net</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>81.960594</td>\n",
              "      <td>81.381971</td>\n",
              "      <td>81.941127</td>\n",
              "      <td>74.621294</td>\n",
              "      <td>87.097997</td>\n",
              "      <td>88.428627</td>\n",
              "      <td>81.320401</td>\n",
              "      <td>81.499791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>89.246382</td>\n",
              "      <td>89.468148</td>\n",
              "      <td>89.359802</td>\n",
              "      <td>79.103060</td>\n",
              "      <td>88.290567</td>\n",
              "      <td>89.839384</td>\n",
              "      <td>81.660581</td>\n",
              "      <td>89.458490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>89.040906</td>\n",
              "      <td>89.349075</td>\n",
              "      <td>89.159708</td>\n",
              "      <td>79.997392</td>\n",
              "      <td>88.955710</td>\n",
              "      <td>90.753500</td>\n",
              "      <td>81.162282</td>\n",
              "      <td>89.369964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>81.527146</td>\n",
              "      <td>81.775397</td>\n",
              "      <td>81.654336</td>\n",
              "      <td>74.767570</td>\n",
              "      <td>82.752588</td>\n",
              "      <td>84.988423</td>\n",
              "      <td>77.617389</td>\n",
              "      <td>81.785427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>87.882694</td>\n",
              "      <td>88.656699</td>\n",
              "      <td>88.262799</td>\n",
              "      <td>78.488034</td>\n",
              "      <td>86.515109</td>\n",
              "      <td>89.021671</td>\n",
              "      <td>82.473952</td>\n",
              "      <td>88.606782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>90.197821</td>\n",
              "      <td>90.180400</td>\n",
              "      <td>90.215872</td>\n",
              "      <td>79.030272</td>\n",
              "      <td>87.199190</td>\n",
              "      <td>89.720010</td>\n",
              "      <td>81.230639</td>\n",
              "      <td>90.207544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>87.387075</td>\n",
              "      <td>87.794687</td>\n",
              "      <td>87.494429</td>\n",
              "      <td>76.348568</td>\n",
              "      <td>86.822274</td>\n",
              "      <td>88.643949</td>\n",
              "      <td>80.375544</td>\n",
              "      <td>87.713386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>89.689415</td>\n",
              "      <td>89.996759</td>\n",
              "      <td>89.835899</td>\n",
              "      <td>82.973267</td>\n",
              "      <td>88.289056</td>\n",
              "      <td>89.962570</td>\n",
              "      <td>81.895664</td>\n",
              "      <td>90.030916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>88.818958</td>\n",
              "      <td>88.720468</td>\n",
              "      <td>88.817974</td>\n",
              "      <td>77.020703</td>\n",
              "      <td>85.718640</td>\n",
              "      <td>88.224966</td>\n",
              "      <td>80.045436</td>\n",
              "      <td>88.788373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>86.168288</td>\n",
              "      <td>86.205717</td>\n",
              "      <td>86.214942</td>\n",
              "      <td>79.022871</td>\n",
              "      <td>85.248371</td>\n",
              "      <td>86.454415</td>\n",
              "      <td>79.943354</td>\n",
              "      <td>86.225121</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Linear Scaled Model      Lasso      Ridge  Decision Tree  Random Forest  \\\n",
              "0            81.960594  81.381971  81.941127      74.621294      87.097997   \n",
              "1            89.246382  89.468148  89.359802      79.103060      88.290567   \n",
              "2            89.040906  89.349075  89.159708      79.997392      88.955710   \n",
              "3            81.527146  81.775397  81.654336      74.767570      82.752588   \n",
              "4            87.882694  88.656699  88.262799      78.488034      86.515109   \n",
              "5            90.197821  90.180400  90.215872      79.030272      87.199190   \n",
              "6            87.387075  87.794687  87.494429      76.348568      86.822274   \n",
              "7            89.689415  89.996759  89.835899      82.973267      88.289056   \n",
              "8            88.818958  88.720468  88.817974      77.020703      85.718640   \n",
              "9            86.168288  86.205717  86.214942      79.022871      85.248371   \n",
              "\n",
              "   Gradient Boosting        KNN  Elastic Net  \n",
              "0          88.428627  81.320401    81.499791  \n",
              "1          89.839384  81.660581    89.458490  \n",
              "2          90.753500  81.162282    89.369964  \n",
              "3          84.988423  77.617389    81.785427  \n",
              "4          89.021671  82.473952    88.606782  \n",
              "5          89.720010  81.230639    90.207544  \n",
              "6          88.643949  80.375544    87.713386  \n",
              "7          89.962570  81.895664    90.030916  \n",
              "8          88.224966  80.045436    88.788373  \n",
              "9          86.454415  79.943354    86.225121  "
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores_adjusted = scores.copy()\n",
        "\n",
        "# for each value, convert to percent\n",
        "\n",
        "for col in scores_adjusted.columns:\n",
        "    scores_adjusted[col] = 100 * (10**scores_adjusted[col] - 1)\n",
        "    # convert to accuracy\n",
        "    scores_adjusted[col] = 100 - scores_adjusted[col]\n",
        "    \n",
        "scores_adjusted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Mean</th>\n",
              "      <th>Std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Gradient Boosting</th>\n",
              "      <td>88.603752</td>\n",
              "      <td>1.741050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Elastic Net</th>\n",
              "      <td>87.368579</td>\n",
              "      <td>3.233688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lasso</th>\n",
              "      <td>87.352932</td>\n",
              "      <td>3.253811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ridge</th>\n",
              "      <td>87.295689</td>\n",
              "      <td>3.121104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Linear Scaled Model</th>\n",
              "      <td>87.191928</td>\n",
              "      <td>3.101599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Random Forest</th>\n",
              "      <td>86.688950</td>\n",
              "      <td>1.802810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>KNN</th>\n",
              "      <td>80.772524</td>\n",
              "      <td>1.371761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Decision Tree</th>\n",
              "      <td>78.137303</td>\n",
              "      <td>2.530867</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          Mean       Std\n",
              "Gradient Boosting    88.603752  1.741050\n",
              "Elastic Net          87.368579  3.233688\n",
              "Lasso                87.352932  3.253811\n",
              "Ridge                87.295689  3.121104\n",
              "Linear Scaled Model  87.191928  3.101599\n",
              "Random Forest        86.688950  1.802810\n",
              "KNN                  80.772524  1.371761\n",
              "Decision Tree        78.137303  2.530867"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# creating a new df with the mean and std of each model\n",
        "\n",
        "scores_mean_std = pd.DataFrame({'Mean': scores_adjusted.mean(),\n",
        "                                'Std': scores_adjusted.std()})\n",
        "scores_mean_std.sort_values(by='Mean', inplace=True, ascending=False)\n",
        "\n",
        "scores_mean_std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Lasso to Linear Scaled Model\n",
            "T-statistic: 0.11\n",
            "p-value: 0.91\n",
            "Results are not significant\n",
            "\n",
            "\n",
            "Comparing Ridge to Linear Scaled Model\n",
            "T-statistic: 0.07\n",
            "p-value: 0.94\n",
            "Results are not significant\n",
            "\n",
            "\n",
            "Comparing Decision Tree to Linear Scaled Model\n",
            "T-statistic: -7.15\n",
            "p-value: 0.00\n",
            "Model is better than baseline\n",
            "\n",
            "\n",
            "Comparing Random Forest to Linear Scaled Model\n",
            "T-statistic: -0.44\n",
            "p-value: 0.66\n",
            "Results are not significant\n",
            "\n",
            "\n",
            "Comparing Gradient Boosting to Linear Scaled Model\n",
            "T-statistic: 1.26\n",
            "p-value: 0.23\n",
            "Model is better than baseline\n",
            "\n",
            "\n",
            "Comparing KNN to Linear Scaled Model\n",
            "T-statistic: -5.99\n",
            "p-value: 0.00\n",
            "Model is better than baseline\n",
            "\n",
            "\n",
            "Comparing Elastic Net to Linear Scaled Model\n",
            "T-statistic: 0.12\n",
            "p-value: 0.90\n",
            "Results are not significant\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "base_line = scores_adjusted['Linear Scaled Model']\n",
        "\n",
        "for col in scores_adjusted.columns:\n",
        "    if col != 'Linear Scaled Model':\n",
        "        print(f'Comparing {col} to Linear Scaled Model')\n",
        "        compare_scores(scores_adjusted[col], base_line)\n",
        "        print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Three models demonstrate superior performance compared to the baseline model:\n",
        "\n",
        "- Decision Tree\n",
        "- Gradient Boosting\n",
        "- k-Nearest Neighbors (kNN)\n",
        "\n",
        "Since Gradient Boosting achieves the highest accuracy among these models, it will be the chosen model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Measuring the performance of the best model on the test set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average error is 12.70%\n",
            "Accuracy is 87.30% +- 0.00%\n"
          ]
        }
      ],
      "source": [
        "final_model = gbr_best\n",
        "\n",
        "# prediciting the test data\n",
        "\n",
        "ypred = final_model.predict(Xtest)\n",
        "\n",
        "# calculating the error\n",
        "\n",
        "error = np.sqrt(mean_squared_error(ytest, ypred))\n",
        "\n",
        "std = error.std()\n",
        "\n",
        "# converting to percent\n",
        "\n",
        "error_percent = 100 * (10**error - 1)\n",
        "\n",
        "std_percent = 100 * (10**std - 1)\n",
        "\n",
        "print(f'Average error is {error_percent:.2f}%')\n",
        "\n",
        "accuracy = 100 - error_percent\n",
        "\n",
        "print(f'Accuracy is {accuracy:.2f}% +- {std_percent:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This model has an accurracy of 87.3 % on the test set, which is very close to the accuracy on the training set (88.6 %). This means that the model is not overfitting the training data, and it is generalizing well to unseen data.\n",
        "\n",
        "Another fact, is that the standard deviation of the accuracy is very low, which means that the model is not sensitive to the data it is given. This is a good sign, since it means that the model will perform well on new data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retreing the model with the full dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      Iter       Train Loss      OOB Improve   Remaining Time \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-17 {color: black;}#sk-container-id-17 pre{padding: 0;}#sk-container-id-17 div.sk-toggleable {background-color: white;}#sk-container-id-17 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-17 label.sk-toggleable__label-arrow:before {content: \"â¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-17 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-17 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-17 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-17 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-17 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-17 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â¾\";}#sk-container-id-17 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-17 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-17 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-17 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-17 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-17 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-17 div.sk-item {position: relative;z-index: 1;}#sk-container-id-17 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-17 div.sk-item::before, #sk-container-id-17 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-17 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-17 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-17 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-17 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-17 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-17 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-17 div.sk-label-container {text-align: center;}#sk-container-id-17 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-17 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-17\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(learning_rate=0.01, max_depth=5, max_features=&#x27;sqrt&#x27;,\n",
              "                          min_samples_leaf=2, n_estimators=1600, subsample=0.6,\n",
              "                          verbose=2, warm_start=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-31\" type=\"checkbox\" checked><label for=\"sk-estimator-id-31\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=0.01, max_depth=5, max_features=&#x27;sqrt&#x27;,\n",
              "                          min_samples_leaf=2, n_estimators=1600, subsample=0.6,\n",
              "                          verbose=2, warm_start=True)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "GradientBoostingRegressor(learning_rate=0.01, max_depth=5, max_features='sqrt',\n",
              "                          min_samples_leaf=2, n_estimators=1600, subsample=0.6,\n",
              "                          verbose=2, warm_start=True)"
            ]
          },
          "execution_count": 137,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_model.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [],
      "source": [
        "# exporting the model\n",
        "\n",
        "path = pathlib.Path.cwd().parent / 'final_model.pkl'\n",
        "\n",
        "with open(path, 'wb') as f:\n",
        "    pickle.dump(final_model, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-19 {color: black;}#sk-container-id-19 pre{padding: 0;}#sk-container-id-19 div.sk-toggleable {background-color: white;}#sk-container-id-19 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-19 label.sk-toggleable__label-arrow:before {content: \"â¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-19 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-19 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-19 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-19 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-19 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-19 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â¾\";}#sk-container-id-19 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-19 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-19 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-19 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-19 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-19 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-19 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-19 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-19 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-19 div.sk-item {position: relative;z-index: 1;}#sk-container-id-19 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-19 div.sk-item::before, #sk-container-id-19 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-19 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-19 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-19 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-19 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-19 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-19 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-19 div.sk-label-container {text-align: center;}#sk-container-id-19 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-19 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-19\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(learning_rate=0.01, max_depth=5, max_features=&#x27;sqrt&#x27;,\n",
              "                          min_samples_leaf=2, n_estimators=1600, subsample=0.6,\n",
              "                          verbose=2, warm_start=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-33\" type=\"checkbox\" checked><label for=\"sk-estimator-id-33\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=0.01, max_depth=5, max_features=&#x27;sqrt&#x27;,\n",
              "                          min_samples_leaf=2, n_estimators=1600, subsample=0.6,\n",
              "                          verbose=2, warm_start=True)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "GradientBoostingRegressor(learning_rate=0.01, max_depth=5, max_features='sqrt',\n",
              "                          min_samples_leaf=2, n_estimators=1600, subsample=0.6,\n",
              "                          verbose=2, warm_start=True)"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# reading the final model\n",
        "\n",
        "path = pathlib.Path.cwd().parent / 'final_model.pkl'\n",
        "\n",
        "with open(path, 'rb') as f:\n",
        "    final_model = pickle.load(f)\n",
        "    \n",
        "final_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Showing the features with the highest importance "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Feature</th>\n",
              "      <th>Importance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Overall.Qual</td>\n",
              "      <td>0.099414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Gr.Liv.Area</td>\n",
              "      <td>0.088036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Total.Bsmt.SF</td>\n",
              "      <td>0.055533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Garage.Area</td>\n",
              "      <td>0.053777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Kitchen.Qual</td>\n",
              "      <td>0.051134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Exter.Qual</td>\n",
              "      <td>0.043274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Fireplaces</td>\n",
              "      <td>0.041951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>X1st.Flr.SF</td>\n",
              "      <td>0.041275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Garage.Cars</td>\n",
              "      <td>0.035893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>House.Age</td>\n",
              "      <td>0.034925</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Feature  Importance\n",
              "4    Overall.Qual    0.099414\n",
              "18    Gr.Liv.Area    0.088036\n",
              "12  Total.Bsmt.SF    0.055533\n",
              "30    Garage.Area    0.053777\n",
              "25   Kitchen.Qual    0.051134\n",
              "7      Exter.Qual    0.043274\n",
              "28     Fireplaces    0.041951\n",
              "15    X1st.Flr.SF    0.041275\n",
              "29    Garage.Cars    0.035893\n",
              "46      House.Age    0.034925"
            ]
          },
          "execution_count": 147,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ferature importance\n",
        "\n",
        "feature_importance = pd.DataFrame({'Feature': data.drop(columns=['SalePrice']).columns,'Importance': final_model.feature_importances_})\n",
        "feature_importance.sort_values(by='Importance', ascending=False, inplace=True)\n",
        "\n",
        "feature_importance.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this model, the most important feature are **Overall Qual**, measurinf the rates the overall material and finish of the house, being and important indicator of the house quality.\n",
        "\n",
        "The second most important feature is **Gr Liv Area**, which measures the above grade (ground) living area square feet.\n",
        "\n",
        "The third most important feature is **Total Bsmt SF**, which measures the total square feet of basement area.\n",
        "\n",
        "The fourth most important feature is **Garage Area**, which measures the garage area in square feet.\n",
        "\n",
        "The fifth most important feature is **Year Built**, which measures the original construction date.\n",
        "\n",
        "The sixth most important feature is **Garage Cars**, which measures the size of garage in car capacity.\n",
        "\n",
        "The seventh most important feature is **1st Flr SF**, which measures the first Floor square feet.\n",
        "\n",
        "The eighth most important feature is **Bsmt Qual**, which measures the height of the basement.\n",
        "\n",
        "The ninth most important feature is **Year Remod/Add**, which measures the remodel date (same as construction date if no remodeling or additions).\n",
        "\n",
        "The tenth most important feature is **Full Bath**, which measures the number of full bathrooms above grade.\n",
        "\n",
        "\n",
        "In conclusion, the most important features are related to the quality of the house, the size of the house, and the year it was built. Also, the model is not sensitive to the data it is given, which means that it will perform well on new data. And we have and accuracy of 87.3 %, which means that the predict value has a 87.3 % chance of being correct, an good insight for the real state company."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
